{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Caption Generator with CNN & LSTM\n\nYou saw an image and your brain can easily tell what the image is about, but can a computer tell what the image is representing? Computer vision researchers worked on this a lot and they considered it impossible until now! With the advancement in Deep learning techniques, availability of huge datasets and computer power, we can build models that can generate captions for an image.\n\nThis is what we are going to implement in this Python based project where we will use deep learning techniques of Convolutional Neural Networks and a type of Recurrent Neural Network (LSTM) together.\n\n## What is Image Caption Generator?\n\nImage caption generator is a task that involves computer vision and natural language processing concepts to recognize the context of an image and describe them in a natural language like English.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Image Caption Generator with CNN – About the Python based Project\n\nThe objective of our project is to learn the concepts of a CNN and LSTM model and build a working model of Image caption generator by implementing CNN with LSTM.\n\nIn this Python project, we will be implementing the caption generator using **CNN (Convolutional Neural Networks)*** and LSTM **(Long short term memory)**. The image features will be extracted from Xception which is a CNN model trained on the imagenet dataset and then we feed the features into the LSTM model which will be responsible for generating the image captions.","metadata":{}},{"cell_type":"markdown","source":"# The Dataset of Python based Project\n\nFor the image caption generator, we will be using the Flickr_8K dataset. There are also other big datasets like Flickr_30K and MSCOCO dataset but it can take weeks just to train the network so we will be using a small Flickr8k dataset. The advantage of a huge dataset is that we can build better models.\n\nwe get dataset from **Kaggle** you can download it from here also : <a href=\"https://www.kaggle.com/datasets/adityajn105/flickr8k\">Kaggle-Flicker8k</a> (Size: 1GB).\n","metadata":{}},{"cell_type":"markdown","source":"#### Let's Begin to code.","metadata":{}},{"cell_type":"markdown","source":"# Import Modules ","metadata":{}},{"cell_type":"code","source":"import os   # handling the files\nimport pickle # storing numpy features\nimport numpy as np\nfrom tqdm.notebook import tqdm # how much data is process till now\n\nfrom tensorflow.keras.applications.vgg16 import VGG16 , preprocess_input # extract features from image data.\nfrom tensorflow.keras.preprocessing.image import load_img , img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical, plot_model\nfrom tensorflow.keras.layers import Input , Dense , LSTM , Embedding , Dropout , add","metadata":{"execution":{"iopub.status.busy":"2024-11-07T09:25:41.795761Z","iopub.execute_input":"2024-11-07T09:25:41.796427Z","iopub.status.idle":"2024-11-07T09:25:47.482929Z","shell.execute_reply.started":"2024-11-07T09:25:41.796322Z","shell.execute_reply":"2024-11-07T09:25:47.482185Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**os** - used to handle files using system commands.\n\n**pickle** - used to store numpy features extracted\n\n**numpy** - used to perform a wide variety of mathematical operations on arrays\n\n**tqdm** - progress bar decorator for iterators. Includes a default range iterator printing to stderr.\n\n**VGG16, preprocess_input** - imported modules for feature extraction from the image data\n\n**load_img, img_to_array** - used for loading the image and converting the image to a numpy array\n\n**Tokenizer** - used for loading the text as convert them into a token\n\n**pad_sequences** - used for equal distribution of words in sentences filling the remaining spaces with zeros\n\n**plot_model** - used to visualize the architecture of the model through different images","metadata":{}},{"cell_type":"markdown","source":"#### Now we must set the directories to use the data","metadata":{}},{"cell_type":"code","source":"BASE_DIR = '/kaggle/input/flickr8k'\nWORKING_DIR = '/kaggle/working'","metadata":{"execution":{"iopub.status.busy":"2024-11-07T09:26:17.630691Z","iopub.execute_input":"2024-11-07T09:26:17.631331Z","iopub.status.idle":"2024-11-07T09:26:17.635293Z","shell.execute_reply.started":"2024-11-07T09:26:17.631293Z","shell.execute_reply":"2024-11-07T09:26:17.634260Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Extract Image Features\n\nWe have to load and restructure the model\n\nVGG-16 is a convolutional neural network that is 16 layers deep. You can load a pretrained version of the network trained on more than a million images from the ImageNet database [1]. The pretrained network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals.","metadata":{}},{"cell_type":"code","source":"# Load vgg16 Model\nmodel = VGG16()\n\n# restructure model\nmodel = Model(inputs = model.inputs , outputs = model.layers[-2].output)\n\n# Summerize\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2024-11-07T09:26:29.937252Z","iopub.execute_input":"2024-11-07T09:26:29.937544Z","iopub.status.idle":"2024-11-07T09:26:51.931836Z","shell.execute_reply.started":"2024-11-07T09:26:29.937510Z","shell.execute_reply":"2024-11-07T09:26:51.931149Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n553467904/553467096 [==============================] - 18s 0us/step\n553476096/553467096 [==============================] - 18s 0us/step\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\nfc2 (Dense)                  (None, 4096)              16781312  \n=================================================================\nTotal params: 134,260,544\nTrainable params: 134,260,544\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"markdown","source":"+ Fully connected layer of the VGG16 model is not needed, just the previous layers to extract feature results.\n\n+ By preference you may include more layers, but for quicker results avoid adding the unnecessary layers.","metadata":{}},{"cell_type":"markdown","source":"# extract the image features\nNow we extract the image features and load the data for preprocess","metadata":{}},{"cell_type":"code","source":"# extract features from image\nfeatures = {}\ndirectory = os.path.join(BASE_DIR, 'Images')\n\nfor img_name in tqdm(os.listdir(directory)):\n    # load the image from file\n    img_path = directory + '/' + img_name\n    image = load_img(img_path, target_size=(224, 224))\n    # convert image pixels to numpy array\n    image = img_to_array(image)\n    # reshape data for model\n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    # preprocess image for vgg\n    image = preprocess_input(image)\n    # extract features\n    feature = model.predict(image, verbose=0)\n    # get image ID\n    image_id = img_name.split('.')[0]\n    # store feature\n    features[image_id] = feature","metadata":{"execution":{"iopub.status.busy":"2024-11-07T09:27:20.125434Z","iopub.execute_input":"2024-11-07T09:27:20.125746Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8091 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34663482cf5f4b5ab1ff43418af5f85e"}},"metadata":{}}]},{"cell_type":"markdown","source":"Dictionary 'features' is created and will be loaded with the extracted features of image data\n\n**load_img(img_path, target_size=(224, 224))** - custom dimension to resize the image when loaded to the array\n\n**image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))** - reshaping the image data to preprocess in a RGB type image.\n\n**model.predict(image, verbose=0)** - extraction of features from the image\n\n**img_name.split('.')[0]** - split of the image name from the extension to load only the image name.","metadata":{}},{"cell_type":"code","source":"# store features in pickle\npickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:45.746917Z","iopub.execute_input":"2022-10-25T08:36:45.748716Z","iopub.status.idle":"2022-10-25T08:36:46.085215Z","shell.execute_reply.started":"2022-10-25T08:36:45.748677Z","shell.execute_reply":"2022-10-25T08:36:46.08444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Extracted features are not stored in the disk, so re-extraction of features can extend running time\n\nDumps and store your dictionary in a pickle for reloading it to save time","metadata":{}},{"cell_type":"code","source":"# load features from pickle\nwith open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n    features = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:46.08648Z","iopub.execute_input":"2022-10-25T08:36:46.086819Z","iopub.status.idle":"2022-10-25T08:36:46.249147Z","shell.execute_reply.started":"2022-10-25T08:36:46.086773Z","shell.execute_reply":"2022-10-25T08:36:46.248386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load all your stored feature data to your project for quicker runtime ","metadata":{}},{"cell_type":"markdown","source":"## Load the Captions Data\n\nLet us store the captions data from the text file","metadata":{}},{"cell_type":"code","source":"with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n    next(f)\n    captions_doc = f.read()","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:46.251763Z","iopub.execute_input":"2022-10-25T08:36:46.252053Z","iopub.status.idle":"2022-10-25T08:36:46.301755Z","shell.execute_reply.started":"2022-10-25T08:36:46.252013Z","shell.execute_reply":"2022-10-25T08:36:46.301006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we split and append the captions data with the image","metadata":{}},{"cell_type":"code","source":"# create mapping of image to captions\nmapping = {}\n# process lines\nfor line in tqdm(captions_doc.split('\\n')):\n    # split the line by comma(,)\n    tokens = line.split(',')\n    if len(line) < 2:\n        continue\n    image_id, caption = tokens[0], tokens[1:]\n    # remove extension from image ID\n    image_id = image_id.split('.')[0]\n    # convert caption list to string\n    caption = \" \".join(caption)\n    # create list if needed\n    if image_id not in mapping:\n        mapping[image_id] = []\n    # store the caption\n    mapping[image_id].append(caption)","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:46.302984Z","iopub.execute_input":"2022-10-25T08:36:46.30334Z","iopub.status.idle":"2022-10-25T08:36:46.425087Z","shell.execute_reply.started":"2022-10-25T08:36:46.303303Z","shell.execute_reply":"2022-10-25T08:36:46.424233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"+ Dictionary 'mapping' is created with key as image_id and values as the corresponding caption text\n\n+ Same image may have multiple captions, **if image_id not in mapping: mapping[image_id] = []** creates a list for appending captions to the corresponding image","metadata":{}},{"cell_type":"markdown","source":"#### Now let us see the no. of images loaded","metadata":{}},{"cell_type":"code","source":"len(mapping)","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:46.426404Z","iopub.execute_input":"2022-10-25T08:36:46.427289Z","iopub.status.idle":"2022-10-25T08:36:46.43505Z","shell.execute_reply.started":"2022-10-25T08:36:46.42725Z","shell.execute_reply":"2022-10-25T08:36:46.43438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess Text Data","metadata":{}},{"cell_type":"code","source":"def clean(mapping):\n    for key, captions in mapping.items():\n        for i in range(len(captions)):\n            # take one caption at a time\n            caption = captions[i]\n            # preprocessing steps\n            # convert to lowercase\n            caption = caption.lower()\n            # delete digits, special chars, etc., \n            caption = caption.replace('[^A-Za-z]', '')\n            # delete additional spaces\n            caption = caption.replace('\\s+', ' ')\n            # add start and end tags to the caption\n            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n            captions[i] = caption","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:46.438245Z","iopub.execute_input":"2022-10-25T08:36:46.43898Z","iopub.status.idle":"2022-10-25T08:36:46.44655Z","shell.execute_reply.started":"2022-10-25T08:36:46.438943Z","shell.execute_reply":"2022-10-25T08:36:46.445702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Defined to clean and convert the text for quicker process and better results","metadata":{}},{"cell_type":"markdown","source":"Let us visualize the text **before** and **after** cleaning","metadata":{}},{"cell_type":"code","source":"# before preprocess of text\nmapping['1000268201_693b08cb0e']","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:46.44765Z","iopub.execute_input":"2022-10-25T08:36:46.448294Z","iopub.status.idle":"2022-10-25T08:36:46.457901Z","shell.execute_reply.started":"2022-10-25T08:36:46.448251Z","shell.execute_reply":"2022-10-25T08:36:46.457088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocess the text\nclean(mapping)","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:46.459857Z","iopub.execute_input":"2022-10-25T08:36:46.460073Z","iopub.status.idle":"2022-10-25T08:36:46.593062Z","shell.execute_reply.started":"2022-10-25T08:36:46.460043Z","shell.execute_reply":"2022-10-25T08:36:46.592436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# after preprocess of text\nmapping['1000268201_693b08cb0e']","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:46.594196Z","iopub.execute_input":"2022-10-25T08:36:46.594438Z","iopub.status.idle":"2022-10-25T08:36:46.599199Z","shell.execute_reply.started":"2022-10-25T08:36:46.594401Z","shell.execute_reply":"2022-10-25T08:36:46.598558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Next we will store the preprocessed captions into a list","metadata":{}},{"cell_type":"code","source":"all_captions = []\nfor key in mapping:\n    for caption in mapping[key]:\n        all_captions.append(caption)","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:46.600321Z","iopub.execute_input":"2022-10-25T08:36:46.600702Z","iopub.status.idle":"2022-10-25T08:36:46.617779Z","shell.execute_reply.started":"2022-10-25T08:36:46.600668Z","shell.execute_reply":"2022-10-25T08:36:46.61713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(all_captions)","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:46.619921Z","iopub.execute_input":"2022-10-25T08:36:46.620295Z","iopub.status.idle":"2022-10-25T08:36:46.631238Z","shell.execute_reply.started":"2022-10-25T08:36:46.620262Z","shell.execute_reply":"2022-10-25T08:36:46.630438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No. of unique captions stored","metadata":{}},{"cell_type":"markdown","source":"# 10 Captions\nLet us see the first ten captions","metadata":{}},{"cell_type":"code","source":"all_captions[:10]","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:46.634305Z","iopub.execute_input":"2022-10-25T08:36:46.634551Z","iopub.status.idle":"2022-10-25T08:36:46.641973Z","shell.execute_reply.started":"2022-10-25T08:36:46.634509Z","shell.execute_reply":"2022-10-25T08:36:46.641224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Processing of Text Data\nNow we start processing the text data","metadata":{}},{"cell_type":"code","source":"# tokenize the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(all_captions)\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:46.64326Z","iopub.execute_input":"2022-10-25T08:36:46.643504Z","iopub.status.idle":"2022-10-25T08:36:47.238979Z","shell.execute_reply.started":"2022-10-25T08:36:46.643474Z","shell.execute_reply":"2022-10-25T08:36:47.238084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:47.240394Z","iopub.execute_input":"2022-10-25T08:36:47.240685Z","iopub.status.idle":"2022-10-25T08:36:47.246678Z","shell.execute_reply.started":"2022-10-25T08:36:47.240625Z","shell.execute_reply":"2022-10-25T08:36:47.24559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No. of unique words","metadata":{}},{"cell_type":"code","source":"# get maximum length of the caption available\nmax_length = max(len(caption.split()) for caption in all_captions)\nmax_length","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:47.248432Z","iopub.execute_input":"2022-10-25T08:36:47.248698Z","iopub.status.idle":"2022-10-25T08:36:47.286034Z","shell.execute_reply.started":"2022-10-25T08:36:47.248664Z","shell.execute_reply":"2022-10-25T08:36:47.285384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"+ Finding the maximum length of the captions, used for reference for the padding sequence.","metadata":{}},{"cell_type":"markdown","source":"# Train Test Split","metadata":{}},{"cell_type":"markdown","source":"#### After preprocessing the data now we will train, test and split","metadata":{}},{"cell_type":"code","source":"image_ids = list(mapping.keys())\nsplit = int(len(image_ids) * 0.90)\ntrain = image_ids[:split]\ntest = image_ids[split:]","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:47.287282Z","iopub.execute_input":"2022-10-25T08:36:47.287517Z","iopub.status.idle":"2022-10-25T08:36:47.293013Z","shell.execute_reply.started":"2022-10-25T08:36:47.287485Z","shell.execute_reply":"2022-10-25T08:36:47.292049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now we will define a batch and include the padding sequence**","metadata":{}},{"cell_type":"code","source":"# create data generator to get data in batch (avoids session crash)\ndef data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n    # loop over images\n    X1, X2, y = list(), list(), list()\n    n = 0\n    while 1:\n        for key in data_keys:\n            n += 1\n            captions = mapping[key]\n            # process each caption\n            for caption in captions:\n                # encode the sequence\n                seq = tokenizer.texts_to_sequences([caption])[0]\n                # split the sequence into X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pairs\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # pad input sequence\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    # encode output sequence\n                    out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]\n                    # store the sequences\n                    X1.append(features[key][0])\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            if n == batch_size:\n                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n                yield [X1, X2], y\n                X1, X2, y = list(), list(), list()\n                n = 0","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:47.29455Z","iopub.execute_input":"2022-10-25T08:36:47.294869Z","iopub.status.idle":"2022-10-25T08:36:47.304955Z","shell.execute_reply.started":"2022-10-25T08:36:47.294834Z","shell.execute_reply":"2022-10-25T08:36:47.303947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Padding sequence normalizes the size of all captions to the max size filling them with zeros for better results.","metadata":{}},{"cell_type":"markdown","source":"# Model Creation","metadata":{}},{"cell_type":"code","source":"# encoder model\n# image feature layers\ninputs1 = Input(shape=(4096,))\nfe1 = Dropout(0.4)(inputs1)\nfe2 = Dense(256, activation='relu')(fe1)\n# sequence feature layers\ninputs2 = Input(shape=(max_length,))\nse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\nse2 = Dropout(0.4)(se1)\nse3 = LSTM(256)(se2)\n\n# decoder model\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# plot the model\nplot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:47.30636Z","iopub.execute_input":"2022-10-25T08:36:47.30663Z","iopub.status.idle":"2022-10-25T08:36:49.354727Z","shell.execute_reply.started":"2022-10-25T08:36:47.306597Z","shell.execute_reply":"2022-10-25T08:36:49.353898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"+ **shape=(4096,)** - output length of the features from the VGG model\n\n+ **Dense** - single dimension linear layer array\n\n+ **Dropout()** - used to add regularization to the data, avoiding over fitting & dropping out a fraction of the data from the layers\n\n+ **model.compile()** - compilation of the model\n\n+ **loss=’sparse_categorical_crossentropy’** - loss function for category outputs\n\n+ **optimizer=’adam’** - automatically adjust the learning rate for the model over the no. of epochs\n\n+ Model plot shows the concatenation of the inputs and outputs into a single layer\n\n+ Feature extraction of image was already done using VGG, no CNN model was needed in this step.","metadata":{}},{"cell_type":"markdown","source":"# Train Model\nNow let us train the model","metadata":{}},{"cell_type":"code","source":"# train the model\nepochs = 20\nbatch_size = 32\nsteps = len(train) // batch_size\n\nfor i in range(epochs):\n    # create data generator\n    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n    # fit for one epoch\n    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:36:49.35648Z","iopub.execute_input":"2022-10-25T08:36:49.357113Z","iopub.status.idle":"2022-10-25T08:59:49.163172Z","shell.execute_reply.started":"2022-10-25T08:36:49.357075Z","shell.execute_reply":"2022-10-25T08:59:49.162465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"+ **steps = len(train) // batch_size** - back propagation and fetch the next data\n\n+ Loss decreases gradually over the iterations\n\n+ Increase the no. of epochs for better results\n\n+ Assign the no. of epochs and batch size accordingly for quicker results\n\n\n### You can save the model in the working directory for reuse","metadata":{}},{"cell_type":"code","source":"# save the model\nmodel.save(WORKING_DIR+'/best_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:59:49.166371Z","iopub.execute_input":"2022-10-25T08:59:49.166744Z","iopub.status.idle":"2022-10-25T08:59:49.298439Z","shell.execute_reply.started":"2022-10-25T08:59:49.166715Z","shell.execute_reply":"2022-10-25T08:59:49.297669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate Captions for the Image","metadata":{}},{"cell_type":"code","source":"def idx_to_word(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:59:49.299818Z","iopub.execute_input":"2022-10-25T08:59:49.300279Z","iopub.status.idle":"2022-10-25T08:59:49.305951Z","shell.execute_reply.started":"2022-10-25T08:59:49.300238Z","shell.execute_reply":"2022-10-25T08:59:49.305226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"+ Convert the predicted index from the model into a word","metadata":{}},{"cell_type":"code","source":"# generate caption for an image\ndef predict_caption(model, image, tokenizer, max_length):\n    # add start tag for generation process\n    in_text = 'startseq'\n    # iterate over the max length of sequence\n    for i in range(max_length):\n        # encode input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        # pad the sequence\n        sequence = pad_sequences([sequence], max_length)\n        # predict next word\n        yhat = model.predict([image, sequence], verbose=0)\n        # get index with high probability\n        yhat = np.argmax(yhat)\n        # convert index to word\n        word = idx_to_word(yhat, tokenizer)\n        # stop if word not found\n        if word is None:\n            break\n        # append word as input for generating next word\n        in_text += \" \" + word\n        # stop if we reach end tag\n        if word == 'endseq':\n            break\n    return in_text","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:59:49.307396Z","iopub.execute_input":"2022-10-25T08:59:49.307653Z","iopub.status.idle":"2022-10-25T08:59:49.317604Z","shell.execute_reply.started":"2022-10-25T08:59:49.307612Z","shell.execute_reply":"2022-10-25T08:59:49.316801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"+ Captiongenerator appending all the words for an image\n\n+ The caption starts with 'startseq' and the model continues to predict the caption until the 'endseq' appeared","metadata":{}},{"cell_type":"markdown","source":"# Model Validation\nNow we validate the data using BLEU Score","metadata":{}},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n# validate with test data\nactual, predicted = list(), list()\n\nfor key in tqdm(test):\n    # get actual caption\n    captions = mapping[key]\n    # predict the caption for image\n    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n    # split into words\n    actual_captions = [caption.split() for caption in captions]\n    y_pred = y_pred.split()\n    # append to the list\n    actual.append(actual_captions)\n    predicted.append(y_pred)\n# calcuate BLEU score\nprint(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\nprint(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","metadata":{"execution":{"iopub.status.busy":"2022-10-25T08:59:49.318852Z","iopub.execute_input":"2022-10-25T08:59:49.319138Z","iopub.status.idle":"2022-10-25T09:05:46.414803Z","shell.execute_reply.started":"2022-10-25T08:59:49.319106Z","shell.execute_reply":"2022-10-25T09:05:46.413909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"+ BLEU Score is used to evaluate the predicted text against a reference text, in a list of tokens.\n\n+ The reference text contains all the words appended from the captions data (actual_captions)\n\n+ A BLEU Score more than **0.4 is considered a good result**, for a better score increase the no. of epochs accordingly.","metadata":{}},{"cell_type":"markdown","source":"## Visualize the Results","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\ndef generate_caption(image_name):\n    # load the image\n    # image_name = \"1001773457_577c3a7d70.jpg\"\n    image_id = image_name.split('.')[0]\n    img_path = os.path.join(BASE_DIR, \"Images\", image_name)\n    image = Image.open(img_path)\n    captions = mapping[image_id]\n    print('---------------------Actual---------------------')\n    for caption in captions:\n        print(caption)\n    # predict the caption\n    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n    print('--------------------Predicted--------------------')\n    print(y_pred)\n    plt.imshow(image)","metadata":{"execution":{"iopub.status.busy":"2022-10-25T09:05:46.418475Z","iopub.execute_input":"2022-10-25T09:05:46.418681Z","iopub.status.idle":"2022-10-25T09:05:46.425676Z","shell.execute_reply.started":"2022-10-25T09:05:46.418656Z","shell.execute_reply":"2022-10-25T09:05:46.424857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"+ Image caption generator defined\n\n+ First prints the actual captions of the image then prints a predicted caption of the image","metadata":{}},{"cell_type":"code","source":"generate_caption(\"1001773457_577c3a7d70.jpg\")","metadata":{"execution":{"iopub.status.busy":"2022-10-25T09:05:46.426759Z","iopub.execute_input":"2022-10-25T09:05:46.427476Z","iopub.status.idle":"2022-10-25T09:05:47.14768Z","shell.execute_reply.started":"2022-10-25T09:05:46.427439Z","shell.execute_reply":"2022-10-25T09:05:47.143756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_caption(\"1002674143_1b742ab4b8.jpg\")","metadata":{"execution":{"iopub.status.busy":"2022-10-25T09:05:47.14905Z","iopub.execute_input":"2022-10-25T09:05:47.149803Z","iopub.status.idle":"2022-10-25T09:05:47.951665Z","shell.execute_reply.started":"2022-10-25T09:05:47.14977Z","shell.execute_reply":"2022-10-25T09:05:47.951003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_caption(\"101669240_b2d3e7f17b.jpg\")","metadata":{"execution":{"iopub.status.busy":"2022-10-25T09:05:47.952994Z","iopub.execute_input":"2022-10-25T09:05:47.953513Z","iopub.status.idle":"2022-10-25T09:05:48.403588Z","shell.execute_reply.started":"2022-10-25T09:05:47.953475Z","shell.execute_reply":"2022-10-25T09:05:48.403012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Thoughts\n\n+ Training the model by increasing the no. of epochs can give better and more accurate results.\n\n+ Processing large amount of data can take a lot of time and system resource.\n\n+ The no. of layers of the model can be increased if you want to process large dataset like flickr32k.\n\n\n\n**In this project , we have built an Image Caption Generator exploring the Flickr Dataset as an advanced deep learning project using different models from image extraction and text based processing.** ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}